Unit: Deep Learning

LONG-ANSWER:
1. Q: Explain how you would design a transformer-based model for time‑series forecasting in your diet planner app. Include architecture and training steps.
 A: Use a 1‑D temporal transformer: embed each time step with a linear layer, add positional encodings, stack 4 encoder layers with multi‑head self‑attention, and a final linear head to predict next‑day nutrient values. Train with AdamW, learning rate 1e‑4, and use a sliding window of 30 days as input. Apply teacher forcing during training and evaluate with MAE. Deploy via Groq for low‑latency inference.

2. Q: In your AI School Management project you used OpenCV for attendance. How would you integrate a CNN for face recognition and what preprocessing steps are critical?
 A: First detect faces with Haar cascades, crop and resize to 224×224, normalize pixel values, and augment with random flips and brightness changes. Feed the preprocessed images into a pre‑trained ResNet‑50 fine‑tuned on the school’s face dataset. Use cosine similarity on the embedding layer for identity matching. Store embeddings in PostgreSQL for quick lookup during attendance logging.

3. Q: Describe the process of fine‑tuning a large language model (LLM) using LangChain for generating personalized diet plans. Include data preparation, prompt design, and evaluation metrics.
 A: Collect user profiles (age, weight, goals) and nutrition guidelines into a JSON dataset. Use LangChain’s PromptTemplate to embed user data into a prompt like "Given {profile}, recommend a 7‑day meal plan adhering to {guidelines}." Fine‑tune a GPT‑4o model on this prompt–response pair set with LoRA adapters. Evaluate with BLEU for text similarity, domain‑specific ROUGE for coverage, and a human‑rated nutrition compliance score.

4. Q: You built a churn predictor using Random Forest. How would you replace it with an LSTM‑based sequence model to capture user behavior over time? Outline data pipeline and loss function.
 A: Transform user logs into sequences of one‑hot encoded events per day. Pad sequences to a fixed length (e.g., 60 days). Feed into an embedding layer followed by two stacked LSTM layers (128 units each). Use a sigmoid output and binary cross‑entropy loss. Train with early stopping on validation AUC. This captures temporal dependencies that Random Forest cannot.

5. Q: In the fraud detection pipeline for cashback bonuses, you used behavioral pattern recognition. How could a graph neural network improve detection? Explain graph construction and message passing.
 A: Construct a user‑transaction graph where nodes are users and transactions, edges represent transaction flows. Encode node features with user demographics and transaction amounts. Apply a GraphSAGE layer to aggregate neighbor information, capturing collusive patterns. Use a final MLP to predict fraud probability. This captures higher‑order relationships beyond pairwise behavior.

6. Q: Discuss how you would implement attention mechanisms in a sequence‑to‑sequence model for translating Japanese PDFs to English summaries. What are the benefits over vanilla seq2seq?
 A: Add a Bahdanau attention layer between the encoder LSTM and decoder LSTM, computing context vectors as weighted sums of encoder hidden states. This allows the decoder to focus on relevant source tokens, improving alignment and reducing exposure bias. Compared to vanilla seq2seq, attention yields higher BLEU scores and better handling of long documents.

7. Q: Your automated reporting uses great_tables and email. Suppose you need to generate real‑time dashboards with live model predictions. Which deep learning framework would you choose and why? Discuss deployment considerations.
 A: Choose TensorFlow Lite for edge inference due to its lightweight runtime and support for quantization, which reduces latency on the server that feeds the Streamlit dashboard. Deploy the model as a REST endpoint behind a FastAPI server, cache predictions in Redis, and stream updates to the dashboard via WebSocket for real‑time interactivity.

8. Q: Explain the role of data augmentation in training a CNN for license plate detection in your parking system. Provide specific augmentation techniques and their impact on model robustness.
 A: Apply random rotations (±15°), scaling (0.9–1.1), brightness jitter, Gaussian blur, and horizontal flips to mimic camera variations. Augmentation increases the effective dataset size, reduces overfitting, and improves recall on occluded or low‑light plates by 8–10% as measured on a held‑out validation set.

9. Q: You have experience with Groq for inference. Compare Groq inference with GPU inference for transformer models in terms of latency and throughput. When would you choose each?
 A: Groq offers sub‑millisecond latency for small to medium transformer models due to its custom tensor‑core architecture, but throughput drops for very large models. GPUs excel at high‑throughput batch inference and support larger models. Use Groq for real‑time API responses; use GPUs for batch training or large‑scale inference jobs.

10. Q: In your resume builder AI, you generate resumes from LinkedIn data. How would you use a transformer encoder to encode job descriptions and match them with user skills? Detail the embedding strategy and similarity metric.
 A: Tokenize job descriptions and user skill lists, feed them into a pre‑trained BERT encoder to obtain CLS embeddings. Compute cosine similarity between job and skill embeddings; rank skills by similarity to recommend the most relevant ones. Fine‑tune the encoder on a small set of manually matched job‑skill pairs to improve domain specificity.

SHORT-ANSWER:
1. Q: What activation function is commonly used in the output layer of a binary classification CNN?
 A: Sigmoid.

2. Q: Which optimizer is best suited for training large language models?
 A: AdamW.

3. Q: Name one library you used for building data pipelines in your current role.
 A: SQLAlchemy.

4. Q: What is the purpose of the softmax function in a multi‑class classification model?
 A: It converts logits into class probabilities.

5. Q: Which metric would you monitor to evaluate a regression model for real estate price prediction?
 A: RMSE or MAE.

6. Q: In your churn predictor, which feature selection technique did you use?
 A: Feature importance from Random Forest.

7. Q: What is the main advantage of using a transformer over an RNN for sequence modeling?
 A: Parallelizable and captures long‑range dependencies.

8. Q: Which database did you use to store processed data for the asset management client?
 A: PostgreSQL.

9. Q: What is the role of the attention mask in transformer training?
 A: It prevents attention to padding tokens.

10. Q: Which Python library did you use for visualizing dashboards in your projects?
 A: Streamlit.
