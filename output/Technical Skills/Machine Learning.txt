Unit: Machine Learning

LONG-ANSWER:
1. Q: Jakir, you built a retention engine using K‑Means clustering and bonus allocation logic. Explain how you would evaluate its effectiveness beyond F‑score and what additional metrics or experiments you would run to ensure it truly improves 30‑day user retention.
 A: I would start by segmenting the user base into treatment and control groups using A/B testing, ensuring random assignment to avoid selection bias. In addition to F‑score, I would track lift in retention rate, churn rate, and average revenue per user (ARPU) over 30 days. Time‑to‑event analysis would reveal how quickly users re‑engage. Finally, I’d monitor for unintended consequences such as increased bonus abuse or churn in high‑value segments, adjusting the clustering thresholds or bonus tiers accordingly.

2. Q: Describe the end‑to‑end pipeline you designed for the Intelligent Document Parsing project. How did you integrate ChatGPT, OCR, and your custom logic to extract tabular data from images?
 A: The pipeline began with image ingestion and pre‑processing (deskewing, contrast enhancement). OCR (Tesseract) extracted raw text, which was then fed to a fine‑tuned ChatGPT model via LangChain to identify table boundaries and interpret cell content. Custom post‑processing scripts parsed the structured output into CSV/JSON, validated schema against expected columns, and stored the results in Snowflake. Error handling included a fallback to manual review for low‑confidence tables.

3. Q: You used Random Forest with hyperparameter tuning for the churn predictor v1. Explain the process you followed for hyperparameter optimization and how you balanced model complexity with interpretability.
 A: I employed a randomized search over a grid of n_estimators, max_depth, min_samples_split, and max_features, using 5‑fold cross‑validation to estimate out‑of‑sample performance. I constrained max_depth to 10 to keep trees shallow, aiding interpretability. Feature importance scores guided pruning of irrelevant features, reducing dimensionality. The final model achieved a 0.78 ROC‑AUC while maintaining a manageable number of trees (200) for explainability.

4. Q: In the Menu Engineering algorithm you used Levenshtein Distance for pattern matching. How did you handle synonyms and contextual variations in menu item names, and what impact did this have on accuracy?
 A: I pre‑processed item names by lowercasing, removing stopwords, and applying stemming. A synonym dictionary (e.g., ‘burger’ ↔ ‘sandwich’) was merged into the token set. During matching, I computed a weighted Levenshtein distance where synonym matches had zero cost. This approach raised accuracy from 70% to 82% by correctly grouping items that differed only in wording or minor typos.

5. Q: Explain how you used the Apriori algorithm for combo analysis in Voosh Food Tech. What were the key metrics you derived, and how did they influence marketing decisions?
 A: I extracted transaction logs from MongoDB, applied the Apriori algorithm to find frequent itemsets with a minimum support of 5%. Confidence and lift metrics identified high‑value combos. These insights guided cross‑sell promotions, reducing the menu‑to‑order conversion gap from 7% to 11.6% by highlighting complementary dishes in the UI.

6. Q: You automated reporting using great_tables and email distribution. Discuss how you ensured data freshness and consistency across the reporting pipeline.
 A: Data ingestion jobs run hourly via Airflow, writing to a versioned staging table in Snowflake. A nightly ETL job aggregates metrics, applies deterministic transformations, and writes to a reporting schema. great_tables renders the latest snapshot, and the email scheduler pulls from the reporting schema, guaranteeing that stakeholders receive the same data snapshot each day. Change data capture logs help trace any discrepancies.

7. Q: Jakir, you have experience with both supervised and unsupervised learning. Provide a scenario where you would choose clustering over classification, and explain the rationale.
 A: When segmenting users for personalized offers without labeled churn data, clustering is appropriate. It uncovers natural groupings based on behavior (e.g., frequency, spend, feature usage). Classification requires labeled outcomes; without them, clustering reveals patterns that can later inform supervised models or targeted campaigns.

8. Q: Describe how you leveraged LangChain and Hugging Face in your Diet Planner application. What challenges did you face with prompt engineering for personalized diet recommendations?
 A: I used LangChain to chain a user profile retrieval step with a generative LLM from Hugging Face. Prompt engineering involved conditioning the LLM on user goals, allergies, and macro targets. Challenges included avoiding hallucinations and ensuring nutritional accuracy; I mitigated this by post‑processing LLM output through a rule‑based validator that cross‑checked macro totals against dietary guidelines.

9. Q: Explain the concept of bias‑variance tradeoff and how you applied it when tuning your fraud detection model for cashback bonuses.
 A: I monitored training vs. validation loss to detect overfitting (high variance) or underfitting (high bias). I reduced model complexity by limiting tree depth and adding regularization, while increasing training data via synthetic samples to lower bias. The final model balanced a low false‑positive rate (to avoid penalizing legitimate users) with a high true‑positive rate (to catch fraud).

10. Q: Jakir, you have built recommendation systems. Outline the steps you would take to evaluate a recommendation algorithm’s performance in a real‑world setting.
 A: First, split data into training, validation, and hold‑out test sets, ensuring temporal integrity. Compute offline metrics like precision@k, recall@k, and NDCG. Then deploy a pilot A/B test, measuring lift in click‑through rate, conversion, and revenue. Finally, monitor drift by tracking changes in user interaction patterns and re‑train periodically.

SHORT-ANSWER:
1. Q: What library did you use for building Random Forest models?
 A: Scikit‑learn.

2. Q: Which database did you integrate for storing processed data in the current role?
 A: PostgreSQL and Snowflake.

3. Q: Name one NLP technique you applied in the Menu Engineering project.
 A: Levenshtein Distance.

4. Q: Which cloud platform did you use for AutoML?
 A: Google Vertex AI.

5. Q: What tool did you use for visualizing dashboards in the Voosh project?
 A: Google Data Studio and Power BI.

6. Q: Which Python library did you use for web scraping?
 A: Selenium, Playwright, Beautiful Soup, Scrapy.

7. Q: What metric did you use to measure churn prediction performance?
 A: F‑score and ROC‑AUC.

8. Q: Which language model did you integrate in the Diet Planner app?
 A: Groq LLM via LangChain.

9. Q: What algorithm did you use for fraud detection in cashback bonuses?
 A: Behavioral pattern recognition with Python (unsupervised clustering).

10. Q: Which database did you use for storing scraped data in Voosh?
 A: MongoDB.
