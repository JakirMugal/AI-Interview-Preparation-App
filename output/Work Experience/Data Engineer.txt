Unit: Data Engineer

LONG-ANSWER:
1. Q: Explain how you would design a unified web‑scraping pipeline that ingests data from multiple APIs and HTML endpoints, ensuring fault tolerance and scalability. Reference your experience at Alternative Path.
 A: I would build a modular pipeline using Scrapy or Playwright for HTML scraping and Requests for APIs, orchestrated by Airflow DAGs. Each source runs in its own task with retry logic and exponential back‑off. Raw data is stored in S3 as compressed Parquet, then processed by a Python ETL job that normalises schemas. For scalability I’d use Docker containers on ECS or Kubernetes, and leverage SQS for decoupling ingestion from processing. Logging and metrics are pushed to CloudWatch for monitoring.

2. Q: Describe the process of transforming unstructured PDF documents into structured data using NLP and image processing, as you did with ChatGPT and OCR. Include steps for handling multilingual content.
 A: First, I extract text with PyMuPDF; for scanned PDFs I run OCR via Tesseract with language packs. I then translate non‑English text using HuggingFace MarianMT models. Sentiment and key‑phrase extraction are performed with spaCy and a fine‑tuned BERT model. Tables are parsed with Camelot or Tabula, and images are processed with OpenCV to detect charts. Finally, I consolidate all outputs into a unified JSON schema and load it into Snowflake via the COPY command.

3. Q: How did you ensure data consistency and optimal query performance when loading data into PostgreSQL and Snowflake using SQLAlchemy ORM? Provide specific techniques.
 A: I used batch inserts with `session.bulk_save_objects` to reduce round‑trips, wrapped operations in transactions, and applied `ON CONFLICT DO UPDATE` for idempotency. For Snowflake I staged data in S3, used the `COPY INTO` command with `FILE_FORMAT = (TYPE = 'PARQUET')`, and defined clustering keys on high‑cardinality columns. Indexes on foreign keys and materialised views for common aggregates further improved query latency.

4. Q: Discuss how you performed downstream data enrichment such as sentiment analysis and timestamp comparison. What libraries and models did you use, and how did you integrate them into the ETL pipeline?
 A: Sentiment analysis was done with HuggingFace’s `distilbert-base-uncased-finetuned-sst-2-english` via the `transformers` pipeline. I applied it to each text field in a Pandas DataFrame, then compared event timestamps using `pandas.Timestamp` arithmetic to flag anomalies. The enriched DataFrame was serialized to Parquet and written to Snowflake. I wrapped the enrichment step in a reusable Python function and scheduled it with Airflow.

5. Q: Explain your approach to automating the distribution of processed data to stakeholders via email, including generating HTML tables and uploading to S3. How did you handle security and scheduling?
 A: I used `great_tables` to convert DataFrames to styled HTML, then templated emails with Jinja2. Emails were sent through AWS SES via `boto3`, with attachments stored in an encrypted S3 bucket. IAM roles restricted access to the bucket and SES. Scheduling was handled by Airflow DAGs that trigger on pipeline completion, ensuring data is fresh and the process is auditable.

6. Q: In your retention engine project at WitZeal, you used K‑Means clustering to segment users. How would you adapt that approach to segment data in a data engineering context, ensuring the pipeline can handle new user data in real‑time?
 A: I would deploy the clustering model as a REST endpoint using FastAPI, and stream new user events via Kafka. Spark Structured Streaming would ingest the stream, apply the model, and write the cluster labels to a Delta Lake table. Incremental training can be scheduled nightly to update centroids, and the updated model is re‑deployed automatically.

7. Q: Describe how you used Snowflake's features (e.g., micro‑partitions, clustering keys) to optimize query performance for the asset management client. Provide examples of query patterns you optimized.
 A: I leveraged micro‑partition pruning by designing tables with appropriate clustering keys on `portfolio_id` and `trade_date`. For time‑series queries I created materialised views on rolling aggregates. I also used Snowflake’s result caching for repeated analytical queries and enabled automatic clustering to keep partitions balanced as data grows.

8. Q: Explain how you integrated external APIs (e.g., CleverTap, WebEngage) into your data pipelines for customer engagement analytics. What challenges did you face and how did you overcome them?
 A: I built a generic API client with pagination handling and exponential back‑off. Authentication was managed via OAuth2 tokens refreshed automatically. Data was normalised into a common schema and stored in Snowflake. Challenges included rate limits and inconsistent field names; I solved these by implementing a mapping layer and caching responses in Redis to reduce API calls.

9. Q: Discuss the security considerations you took into account when handling sensitive data (e.g., email attachments, financial data) during ingestion and storage. How did you implement encryption and access controls?
 A: All S3 buckets used SSE‑KMS with customer‑managed keys. IAM policies granted least‑privilege access to only the ingestion Lambda functions. PostgreSQL data was encrypted at rest using pgcrypto and TLS for transit. I also implemented row‑level security in Snowflake to restrict access to sensitive columns.

10. Q: Reflect on your experience building automated dashboards with Power BI and Google Data Studio. How would you design a data pipeline that feeds real‑time metrics into these tools, ensuring data freshness and minimal latency?
 A: I set up a Snowflake data sharing endpoint that Power BI and Data Studio can query directly. Incremental loads are scheduled every 15 minutes via Airflow, and I use Snowpipe for continuous ingestion. For Power BI I enable DirectQuery to avoid caching delays, while for Data Studio I use the BigQuery connector with scheduled refreshes.

11. Q: What best practices do you follow when designing ETL pipelines that involve both structured and unstructured data sources?
 A: I adopt a modular architecture with separate ingestion, transformation, and loading stages. I use schema‑on‑read for unstructured data, store raw files in a data lake, and apply data profiling to detect anomalies. Logging, unit tests, and data lineage tracking with tools like Airflow’s XComs ensure traceability and maintainability.

SHORT-ANSWER:
1. Q: What Python libraries do you use for web scraping?
 A: Selenium, Playwright, BeautifulSoup, Requests, and Scrapy.

2. Q: Which database systems are you comfortable with?
 A: PostgreSQL, Snowflake, MySQL, and MongoDB.

3. Q: How do you handle schema evolution in a data lake?
 A: Use schema‑on‑read with Avro/Parquet and maintain a versioned schema registry.

4. Q: What tool did you use for email automation in your current role?
 A: Python’s smtplib with boto3 for SES, combined with great_tables for HTML tables.

5. Q: Which cloud platform did you use for AutoML?
 A: Google Vertex AI.

6. Q: What is your experience with SQLAlchemy?
 A: ORM for PostgreSQL and Snowflake, batch inserts, session management, and migrations.

7. Q: How do you perform sentiment analysis in your pipelines?
 A: Using HuggingFace transformers (e.g., DistilBERT) or TextBlob integrated via Pandas.

8. Q: What is the purpose of using Snowflake's micro‑partitions?
 A: They enable automatic data clustering and efficient pruning for faster queries.

9. Q: Which framework did you use for building the diet planner app?
 A: Streamlit.

10. Q: How do you ensure data quality during ETL?
 A: Validation checks, logging, unit tests, and data profiling with tools like Great Expectations.
